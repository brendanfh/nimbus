package nimbus

use runtime

#package
TCP_Server :: struct {
    nimbus: &Nimbus

    listen_socket: net.Socket
    listen_address: str

    thread_pool: [] thread.Thread

    ready_sockets: sync.Channel(& net.SocketAcceptResult)
}

TCP_Settings :: struct {
    max_clients := 32
    thread_count := 0
    address := "0.0.0.0"
}

#package
tcp_server_make :: (nimbus: &Nimbus, settings: TCP_Settings) -> &TCP_Server {
    threads := make([] thread.Thread, settings.thread_count, nimbus.allocator)

    listen_socket := net.socket_create(.Inet, .Stream, .IP).Ok ?? [] {
        return #from_proc null
    }

    server := new(TCP_Server.{
        nimbus = nimbus
        thread_pool = threads,
        listen_socket = listen_socket
        listen_address = settings.address
        ready_sockets = sync.Channel.make(& net.SocketAcceptResult)
    }, nimbus.allocator)

    for& t in threads {
        thread.spawn(t, server, tcp_server_thread)
    }

    return server
}



Serve_Status :: enum {
    Success
    Failed_To_Bind
}

TCP_Server.serve :: (tcp: &TCP_Server, port: u32, addr := "0.0.0.0") -> Serve_Status {
    // Allows the server to be immediately restarted. Not
    // the safest, but it is fine for a development instance.
    tcp.listen_socket->option(.ReuseAddress, true)

    listen_address: net.SocketAddress
    if !do {
        net.make_ipv4_address(&listen_address, tcp.listen_address, ~~ port)
        if !tcp.listen_socket->bind(&listen_address) do return false
        if !tcp.listen_socket->listen() do return false
        return true
    } {
        log(.Error, "Nimbus", core.tprintf("Failed to bind to port {}.\n", port))
        return .Failed_To_Bind
    }
    
    logf(.Info, "Serving on port {}", port)
    while true {
        make_sure_all_threads_are_alive(tcp)

        tcp.listen_socket->accept().Ok->with([conn] {
            if !tcp.thread_pool {
                handle_request(tcp, &conn)

            } else {
                conn_on_heap := new(conn)
                tcp.ready_sockets->send(conn_on_heap)
            }
        })
    }

    return .Success
}

#local
make_sure_all_threads_are_alive :: (tcp: &TCP_Server) {
    for &t in tcp.thread_pool {
        if !t.alive {
            logf(.Info, "Spawning another thread because thread {} died", t.id)
            thread.spawn(t, tcp, tcp_server_thread)
        }
    }
}

#local
tcp_server_thread :: (tcp: &TCP_Server) {
    while true {
        conn := tcp.ready_sockets->recv()?
        handle_request(tcp, conn)
        context.allocator->free(conn)
    }
}

#local
handle_request :: (tcp: &TCP_Server, conn: &net.SocketAcceptResult) {
    result := inner_handle_request(tcp, conn) 
    switch result {
        case .Ok do net.socket_close(&conn.socket)
        case .DeadSocket, .SwitchedProtocols do return
        case .BadRequest {
            io.stream_write(&conn.socket, "HTTP/1.1 500 Internal Server Error\r\nConnection: close\r\n\r\n")
            net.socket_close(&conn.socket)
        }
    }
}


#local
InnerHandleRequestResult :: enum {
    Ok
    DeadSocket
    BadRequest
    SwitchedProtocols
}

#local
inner_handle_request :: (tcp: &TCP_Server, conn: &net.SocketAcceptResult) -> InnerHandleRequestResult {
    // If the stream is no longer set up correctly, abandon it.
    if !conn.socket.alive || conn.socket.vtable == null {
        return .DeadSocket
    }

    session_gc := alloc.gc.make()
    use_gc_allocator_in_current_scope(&session_gc)

    // We have to copy the client's socket because the request handler
    // has the ability to call TCP_Server.Client.detach which frees the
    // client and releases control of the socket to the caller. This
    // enables WebSockets and similar protocols. However, here we still
    // need to respond to the request, so we must copy the socket to
    // use it later.
    client_addr := conn.addr
    client_sock := conn.socket

    alloc.clear_temp_allocator()

    c := make_context(tcp.nimbus, &client_sock, context.allocator)
    c.socket = client_sock
    defer destroy_context(&c)

    req := c.req
    req.address = .{ client_addr->addr_as_str() }

    use reader := io.reader_make(&client_sock)
    if !parse_request(req, &reader) {
        return .BadRequest
    }

    // Process the request!
    process_request(tcp.nimbus, ~~ &c)

    // If the body was not read for some reason,
    // we need to read it before we can read the next request.
    // 
    // This is not necessary if we close the connection right away
    // if !req._body_was_read {
    //     request_parse_raw(req)
    // }

    if c.res.status == 101 {
        return .SwitchedProtocols
    }

    return .Ok
}


#local
parse_request :: (req: &Request, r: &io.Reader) -> bool {
    request_line := r->read_line()
    if request_line.length == 0 do return false

    method, request_line~ := str.bisect(request_line, ' ')

    req.method = runtime.info.enum_values(HTTP_Method)
        |> Array.find_opt([v](v.name == method))
        |> Optional.transform(x => cast(HTTP_Method) x.value)
        |> Optional.value_or(.UNKNOWN)

    route, request_line~ := str.bisect(request_line, ' ')

    if str.starts_with(route, "http://") || str.starts_with(route, "https://") {
        str.read_until(&route, "/", 2)
    }

    query_params: _
    req.endpoint, query_params = str.bisect(route, '?')

    req._processed_endpoint = process_url_to_route_elems(req.endpoint, context.allocator)

    parse_url_encoded_key_value(&req.query, &query_params)

    while true {
        header, value := parse_header(r)
        if header == "" do break

        if header == "cookie" {
            // Parse cookies
            parse_cookies(value, &req.cookies)
            continue
        }

        req.headers[header] = value
    }

    req.body_reader = r
    return true
}



#local
use_gc_allocator_in_current_scope :: macro (gc: &alloc.gc.GCState) {
    old_allocator := context.allocator
    context.allocator = alloc.as_allocator(gc)

    defer {
        alloc.gc.clear(gc)
        context.allocator = old_allocator
    }
}

